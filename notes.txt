- The Enron email database is processed from the initial untarred
  directory tree with email files at the leafs by using scripts in
  src/bash_scripts:

     Step 1: using createCleanEnronCollection.sh, create two sets of
       files: one just the raw emails chunked into  
       50MB files, with separators in between, and a sister set with
       those messages cleaned up (numbers, special chars, URLs, email 
       headers removed). The raw set are called email<n>.txt, the latter
       email<n>_NoHeads.txt. They will appear in 
       src/echo_tree/Resources/EnronCollectionProcessed/EnronRaw, and
       src/echo_tree/Resources/EnronCollectionProcessed/EnronCleaned, and
       Assumption: collection is in $HOME/Project/Dreslconsulting/Data/Enron/enron_mail_20110402/maildir/

           ./createCleanEnronCollection.sh 

     Step2: using tokenizeCleanEmails.sh, create arrays of tokens, one
       array for each sentence. Result will be in 
       src/echo_tree/Resources/EnronCollectionProcessed/EnronTokenized.

           ./tokenizeCleanEmails.sh

       Tokenization fails in some cases, and must be fixed in each
       file:

          * Find a bad spot where a closing bracket isn't followed
            immediately by an opening bracket:
	        M-x find-forward-regexp \][^[]      // shortcut M-s
            Look back to find the most recent good one:
	        cnt-r ][
            Set point (cnt-space), and do the same search forward:
                cnt-s cnt-s
            Delete the trash in between.
          * M-x replace-string ^M with <nothing>
          * M-x replace-string ^J with <nothing>

     Step3: Using the set of tokenized sentence files of Step2, create
     a CSV file with the following columns:

          "word,follower,followingCount,metaTotalOcc,metaNumSuccessors,metaWordLen"

     word: one word from an email; stopwords eliminated (see list of
           stopwords below). This col is *not* unique.
     follower: one word that followed Word on one occasion.
     followingCount: number of times 'follower' followed 'word'
     metaTotalOcc: total number of occurrences of 'word'
     metaNumSuccessors: number of successors to 'word'
     metaWordLen: length of 'word'

     For each word there is exactly one entrance with the Metaxxx
     columns filled in. That row has empty Follower and FollowersCount
     columns. There are then separate rows for each Word/Follower
     pair, in which the Metaxxx columns are empty

     Create theis csv like this:

      ./make_database_from_emails.py -o <outputCSVFile> <dirWithTokenizedSentences>

     Within the source tree I did:

      mkdir Resources/EnronCollectionProcessed/EnronDB
      ./make_database_from_emails.py -o \
                      Resources/EnronCollectionProcessed/EnronDB/enronDB.csv \
		      Resources/EnronCollectionProcessed/EnronTokenized
 
     This run takes 8 hours on an 8-core/16GB machine. However, the
     data structure built in memory throws away two expensive lists
     for each words that don't make it into any csv file: for each
     word the program creates a list of unique sentence IDs and unique
     email IDs in which that word occurred. Only those lists' length
     is written to the csv file (MetaNumSentenceOcc,
     MetaNumMsgOcc). Only collecting those counts would presumably
     speed up the db creation.

   Step5: Due to a bug, 465 lines in enronDB.csv were not proper
          csv entries, but raw text. I eliminated them via this
          script

	         #!/bin/bash
                 # -i replaces matches in place, i.e. within the file:
	         sed -i '
	            # If line has exactly five commas with non-commas
	  	  # in between, then that line is good: go to the next 
	            # line. Else fall through to the delete-line ('d'):
	            /[^,]*,[^,]*,[^,]*,[^,]*,[^,]*,/n
	            d
	            '
	         $HOME/fuerte/stacks/echo_tree/src/echo_tree/Resources/EnronCollectionProcessed/EnronDB/enronDB.csv

   Step6: Database creation command:

       In sqliteScript (a compiled version of sqlite3 with large files
       turned on; see worstation notes.). Schema:

       create table EnronWords (
          word text,
          follower text,
          followingCount int,
	  metaTotalOcc int,
          metaNumSuccessors int,
          metaWordLen int
       );            


   Step7: Import into SQLite:
        * In RazorSQL: do Import->CSV
				Separator: <comma>
				First-line = 2
				Don't include header names
				Do pad missing values with null
                        Output SQL to a file only: csvImportStatements.sql

	* In Emacs:     add 
			   BEGIN TRANSACTION;
			   COMMIT TRANSACTION;
                              respectivly to the start and end fo the buffer,
                        remove lines that only contain a
			semicolon. Those come from bad entries in
			token files.
			   M-x replace-regexp
			      ^;
			      <enter>
                        This leaves some empty lines; no prob.

 	* In shell:     sqliteShell enronDB.db < csvImportStatements.sql
                           takes a few minutes.
       
   Step8: Create an index:

        * Using RazorSQL or sqLiteShell:
              CREATE INDEX wordFollCountIndx 
   	         ON EnronWords (word, followingCount);
   

- The tokenization/sentence segmentation is done using Stanford NLP
  core. Separate package: echo_tree_sentence_seg. Use Eclipse over
  that Maven project to create target/emailTokenizer.jar. Copy that
  jar file into ... echo_tree/src/bash_scripts. (Already done in a
  cloned project.

- Enron data:
    o 48,900 messages
    o 466,837 distinct words
    o 2.624GB of total length of email files
    o csv file 174,465,277bytes == 174GB uncompressed (compressed: 4,6015,851)
    o csv file has 7,890,661 rows (that's larger than the word
        count!)
    o Final DB:
       * 3,881,632 rows (not same as unique words!)

- Stopwords 
  Selected from the 100 pre billion most frequent English words
  at http://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/PG/2006/04/1-10000

STOPWORDS =  ['a','able','about','across','after','all','almost','also','am','among','an','and','any',
              'are','as','at','be','because','been','but','by','can','cannot','could','dear','did','do',
              'does','either','else','ever','every','for','from','get','got','had','has','have','he',
              'her','hers','him','his','how','however','i','if','in','into','is','it','its','just',
              'least','let','like','likely','may','me','might','most','must','my','neither','no','nor',
              'not','of','off','often','on','only','or','other','our','own','rather','said','say','says',
              'she','should','since','so','some','than','that','the','their','them','then','there','these',
              'they','this','tis','to','too','twas','us','wants','was','we','were','what','when','where',
              'which','while','who','whom','why','will','with','would','yet','you','your',
              'Subject', 'cc', 'bcc', 'nbspb', 'mr.', 'inc.', 'one', 'two', 'three', 'four', 'five', 
              'six', 'seven', 'eight', 'nine', 'ten', 'enron', 'http'];


More stopword candidates:
     with, in, or, from, 

.mode csv
.import enronDB.csv EnronWords



----------------------------------


- Using the Stanford NLP sentence segmenter and tokenizer:
     java edu.stanford.nlp.process.DocumentPreprocessor -file myDoc.txt > oneTokenizedSentencePerLine.txt
        -file for Text file, -html for HTML, etc.


Result of stringifying in JSON. This is what Python server
must generate and send over the WebSocket wire:
{
    "word": "great",
    "followWordObjs": [
        {
            "word": "keyboard",
            "followWordObjs": [
                {
                    "word": "shortcut",
                    "followWordObjs": [
                        {
                            "word": "lost",
                            "followWordObjs": []
                        },
                        {
                            "word": "woods",
                            "followWordObjs": []
                        }
                    ]
                },
                {
                    "word": "failure",
                    "followWordObjs": [
                        {
                            "word": "mode",
                            "followWordObjs": []
                        },
                        {
                            "word": "option",
                            "followWordObjs": []
                        }
                    ]
                },
                {
                    "word": "cleaner",
                    "followWordObjs": [
                        {
                            "word": "undocumented",
                            "followWordObjs": []
                        },
                        {
                            "word": "acidic",
                            "followWordObjs": []
                        }
                    ]
                }
            ]
        },
        {
            "word": "land",
            "followWordObjs": [
                {
                    "word": "free",
                    "followWordObjs": [
                        {
                            "word": "potatoes",
                            "followWordObjs": []
                        },
                        {
                            "word": "wheeling",
                            "followWordObjs": []
                        }
                    ]
                },
                {
                    "word": "reform",
                    "followWordObjs": [
                        {
                            "word": "school",
                            "followWordObjs": []
                        },
                        {
                            "word": "attempt",
                            "followWordObjs": []
                        }
                    ]
                },
                {
                    "word": "dispute",
                    "followWordObjs": [
                        {
                            "word": "defendant",
                            "followWordObjs": []
                        },
                        {
                            "word": "allegations",
                            "followWordObjs": []
                        }
                    ]
                }
            ]
        },
        {
            "word": "labor",
            "followWordObjs": [
                {
                    "word": "movement",
                    "followWordObjs": [
                        {
                            "word": "free",
                            "followWordObjs": []
                        },
                        {
                            "word": "therapy",
                            "followWordObjs": []
                        }
                    ]
                },
                {
                    "word": "union",
                    "followWordObjs": [
                        {
                            "word": "concerned",
                            "followWordObjs": []
                        },
                        {
                            "word": "denied",
                            "followWordObjs": []
                        }
                    ]
                },
                {
                    "word": "relations",
                    "followWordObjs": [
                        {
                            "word": "bad",
                            "followWordObjs": []
                        },
                        {
                            "word": "visit",
                            "followWordObjs": []
                        }
                    ]
                }
            ]
        }
    ]
}
----------------------

{"word": "echo", 
 "followWordObjs": [
	    		{"word": "chamber", 
	    		 "followWordObjs": [
	    		 		   {"word": "music", 
	    				    "followWordObjs": []}, 
	    				   {"word": "orchestra", 
	    				    "followWordObjs": []}
	    				   ]
	    	        }, 
	    		{"word": "measurement", 
	    		 "followWordObjs": []
	    		}, 
	    		{"word": "deafening", 
	    		 "followWordObjs": []}
		  ]
}
