- The Enron email database is processed from the initial untarred
  directory tree with email files at the leafs by using scripts in
  src/bash_scripts:

     Step 1: using createCleanEnronCollection.sh, create two sets of
       ffiles: one just the raw emails chunked into  
       50MB files, with separators in between, and a sister set with
       those messages cleaned up (numbers, special chars, URLs, email 
       headers removed). The raw set are called email<n>.txt, the latter
       email<n>_NoHeads.txt. They will appear in 
       src/echo_tree/Resources/EnronCollectionProcessed/EnronRaw, and
       src/echo_tree/Resources/EnronCollectionProcessed/EnronCleaned, and
       Assumption: collection is in $HOME/Project/Dreslconsulting/Data/Enron/enron_mail_20110402/maildir/

           ./createCleanEnronCollection.sh 

     Step2: using tokenizeCleanEmails.sh, create arrays of tokens, one
       array for each sentence. Result will be in 
       src/echo_tree/Resources/EnronCollectionProcessed/EnronTokenized.

           ./tokenizeCleanEmails.sh

     Step3: Using the set of tokenized sentence files of Step2, create
     a CSV file with the following columns:

          "Word,Follower,FollowersCount,MetaNumSuccessors,MetaNumSentenceOcc,MetaNumMsgOcc"

     Word: one word from an email; stopwords eliminated (see list of
           stopwords below). This col is *not* unique.
     Follower: one word that followed Word on one occasion.
     FollowersCount: number of distinct words that followed Word
     MetaNumSuccessors: total number of distinct successors to Word
     MetaNumSentenceOcc: number of sentences in which Word occurs
     MetaNumMsgOcc: number of email messages in which Word occurred

     For each word there is exactly one entrance with the Metaxxx
     columns filled in. That row has empty Follower and FollowersCount
     columns. There are then separate rows for each Word/Follower
     pair, in which the Metaxxx columns are empty

     Create theis csv like this:

      ./make_database_from_emails.py -o <outputCSVFile> <dirWithTokenizedSentences>

     Within the source tree I did:

      mkdir Resources/EnronCollectionProcessed/EnronDB
      ./make_database_from_emails.py -o \
                      Resources/EnronCollectionProcessed/EnronDB/enronDB.csv \
		      Resources/EnronCollectionProcessed/EnronDB/enronTokenized
 
     This run takes 8 hours on an 8-core/16GB machine. However, the
     data structure built in memory throws away two expensive lists
     for each words that don't make it into any csv file: for each
     word the program creates a list of unique sentence IDs and unique
     email IDs in which that word occurred. Only those lists' length
     is written to the csv file (MetaNumSentenceOcc,
     MetaNumMsgOcc). Only collecting those counts would presumably
     speed up the db creation.

   [Step4: Due to a bug that is now fixed, an additional comma was in
          the original csv. Any newly created csv file does not need
          this step:

       #!/bin/bash
       sed -i 's/,,,,$/,,,/g' $HOME/fuerte/stacks/echo_tree/src/echo_tree/Resources/EnronCollectionProcessed/EnronDB/enronDB.csv
   ]
   Step5: Due to a bug, 465 lines in enronDB.csv were not proper
          csv entries, but raw text. I eliminated them via this
          script

	         #!/bin/bash
                 # -i replaces matches in place, i.e. within the file:
	         sed -i '
	            # If line has exactly five commas with non-commas
	  	  # in between, then that line is good: go to the next 
	            # line. Else fall through to the delete-line ('d'):
	            /[^,]*,[^,]*,[^,]*,[^,]*,[^,]*,/n
	            d
	            '
	         $HOME/fuerte/stacks/echo_tree/src/echo_tree/Resources/EnronCollectionProcessed/EnronDB/enronDB.csv
          Also, several individual entries confused SQlite. Ex: 
          Fixes along that line: deletes:
                 sed -i '3595077d' $HOME/fuerte/stacks/echo_tree/src/echo_tree/Resources/EnronCollectionProcessed/EnronDB/enronDB.csv

          Other lines: 
                 3665954
		 3665953

       

- The tokenization/sentence segmentation is done using Stanford NLP
  core. Separate package: echo_tree_sentence_seg. Use Eclipse over
  that Maven project to create target/emailTokenizer.jar. Copy that
  jar file into ... echo_tree/src/bash_scripts. (Already done in a
  cloned project.

- Enron data:
    o 531,422 messages
    o 2.624GB of total length of email files
    o enronDB has 7,332,416 entries (that's larger than the word
        count!)

- Full run:
    Processed 517424 emails (1421.000 MB)
    Mon Sep  3 12:27:35 PDT 2012

- Stopwords 
  Selected from the 100 pre billion most frequent English words
  at http://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/PG/2006/04/1-10000
the
of
and
to
in
I
that
was
his
he
it
is
for
as
had
on
at
by
this
are
an
has
its
a
these
mr

- Database creation command:
create table EnronWords (
   word text,
   follower text,
   followersCount int,
   metaNumSuccessors int,
   metaNumSentenceOcc int,
   metaNumMsgOcc int
);

.mode csv
.import enronDB.csv EnronWords



----------------------------------


- Using the Stanford NLP sentence segmenter and tokenizer:
     java edu.stanford.nlp.process.DocumentPreprocessor -file myDoc.txt > oneTokenizedSentencePerLine.txt
        -file for Text file, -html for HTML, etc.


Result of stringifying in JSON. This is what Python server
must generate and send over the WebSocket wire:
{
    "word": "great",
    "followWordObjs": [
        {
            "word": "keyboard",
            "followWordObjs": [
                {
                    "word": "shortcut",
                    "followWordObjs": [
                        {
                            "word": "lost",
                            "followWordObjs": []
                        },
                        {
                            "word": "woods",
                            "followWordObjs": []
                        }
                    ]
                },
                {
                    "word": "failure",
                    "followWordObjs": [
                        {
                            "word": "mode",
                            "followWordObjs": []
                        },
                        {
                            "word": "option",
                            "followWordObjs": []
                        }
                    ]
                },
                {
                    "word": "cleaner",
                    "followWordObjs": [
                        {
                            "word": "undocumented",
                            "followWordObjs": []
                        },
                        {
                            "word": "acidic",
                            "followWordObjs": []
                        }
                    ]
                }
            ]
        },
        {
            "word": "land",
            "followWordObjs": [
                {
                    "word": "free",
                    "followWordObjs": [
                        {
                            "word": "potatoes",
                            "followWordObjs": []
                        },
                        {
                            "word": "wheeling",
                            "followWordObjs": []
                        }
                    ]
                },
                {
                    "word": "reform",
                    "followWordObjs": [
                        {
                            "word": "school",
                            "followWordObjs": []
                        },
                        {
                            "word": "attempt",
                            "followWordObjs": []
                        }
                    ]
                },
                {
                    "word": "dispute",
                    "followWordObjs": [
                        {
                            "word": "defendant",
                            "followWordObjs": []
                        },
                        {
                            "word": "allegations",
                            "followWordObjs": []
                        }
                    ]
                }
            ]
        },
        {
            "word": "labor",
            "followWordObjs": [
                {
                    "word": "movement",
                    "followWordObjs": [
                        {
                            "word": "free",
                            "followWordObjs": []
                        },
                        {
                            "word": "therapy",
                            "followWordObjs": []
                        }
                    ]
                },
                {
                    "word": "union",
                    "followWordObjs": [
                        {
                            "word": "concerned",
                            "followWordObjs": []
                        },
                        {
                            "word": "denied",
                            "followWordObjs": []
                        }
                    ]
                },
                {
                    "word": "relations",
                    "followWordObjs": [
                        {
                            "word": "bad",
                            "followWordObjs": []
                        },
                        {
                            "word": "visit",
                            "followWordObjs": []
                        }
                    ]
                }
            ]
        }
    ]
}
----------------------

{"word": "echo", 
 "followWordObjs": [
	    		{"word": "chamber", 
	    		 "followWordObjs": [
	    		 		   {"word": "music", 
	    				    "followWordObjs": []}, 
	    				   {"word": "orchestra", 
	    				    "followWordObjs": []}
	    				   ]
	    	        }, 
	    		{"word": "measurement", 
	    		 "followWordObjs": []
	    		}, 
	    		{"word": "deafening", 
	    		 "followWordObjs": []}
		  ]
}
